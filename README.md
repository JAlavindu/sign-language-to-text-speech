# ğŸ§¤ Sign Language to Text & Speech System

> **A Multi-Modal Approach to ASL Translation using Computer Vision and Wearable Sensors**

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)
![OpenCV](https://img.shields.io/badge/OpenCV-Computer%20Vision-green)
![ESP32](https://img.shields.io/badge/Hardware-ESP32-red)
![Status](https://img.shields.io/badge/Status-In%20Development-yellow)

## ğŸ“– About The Project

This project aims to bridge the communication gap for the deaf and hard-of-hearing community by translating American Sign Language (ASL) into text and spoken speech in real-time.

Unlike traditional systems that rely solely on cameras _or_ gloves, this project implements a **Dual-Input System**:

1.  **The Eyes (Camera)**: Uses Computer Vision (MediaPipe + CNN) to recognize hand shapes visually.
2.  **The Feel (Glove)**: Uses Flex Sensors and IMUs to capture finger bending and hand motion.
3.  **The Voice (TTS)**: Converts recognized gestures into spoken words using Text-to-Speech.

## âœ¨ Key Features

- **Real-Time Recognition**: Instant translation of ASL signs via Webcam.
- **Smart Smoothing**: Temporal smoothing algorithms to prevent jittery predictions.
- **Text-to-Speech (TTS)**: Speaks the recognized sentence out loud.
- **Sentence Building**: Automatically constructs sentences when signs are held stable.
- **Wireless Glove**: ESP32-based wearable streaming sensor data via Bluetooth Low Energy (BLE).
- **Custom Training Pipeline**: Complete scripts to train your own models on custom datasets.

## ğŸ“‚ Project Structure

```
sign-language-glove/
â”œâ”€â”€ docs/                   # Documentation & Guides
â”œâ”€â”€ firmware/               # ESP32 Microcontroller Code
â”‚   â””â”€â”€ sensor_streamer/    # BLE Sensor Streaming Firmware
â”œâ”€â”€ hardware/               # Wiring Diagrams & Parts Lists
â”œâ”€â”€ ml-model/               # Machine Learning Core
â”‚   â”œâ”€â”€ datasets/           # Raw & Processed Data
â”‚   â”œâ”€â”€ models/             # Trained .h5 Models
â”‚   â”œâ”€â”€ reports/            # Generated Graphs & Confusion Matrices
â”‚   â”œâ”€â”€ utils/              # Helper Modules (HandDetector, Smoother, Fusion, etc.)
â”‚   â”œâ”€â”€ 1_data_exploration.py
â”‚   â”œâ”€â”€ 2_prepare_dataset.py
â”‚   â”œâ”€â”€ 3_train_model.py
â”‚   â”œâ”€â”€ 7_realtime_camera.py      # ğŸ“· Camera-Only Demo
â”‚   â”œâ”€â”€ 8_collect_sensor_data.py  # ğŸ§¤ Sensor Data Collector
â”‚   â”œâ”€â”€ 9_train_sensor_model.py   # ğŸ§  Train Sensor Model
â”‚   â”œâ”€â”€ 10_realtime_sensor.py     # ğŸ§¤ Sensor-Only Demo
â”‚   â”œâ”€â”€ 11_multimodal_fusion.py   # ğŸš€ Hybrid System Demo
â”‚   â”œâ”€â”€ 12_final_app.py           # ğŸ† CLI Product (TTS + Sentence Builder)
â”‚   â”œâ”€â”€ 13_generate_report_graphs.py # ğŸ“Š Generate Report Artifacts
â”‚   â”œâ”€â”€ 14_gui_app.py             # ğŸ–¥ï¸ Professional GUI Product
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ setup_ml.bat            # Windows Setup Script
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

- **Software**: Python 3.10 or 3.11 (Recommended for MediaPipe compatibility).
- **Hardware**: Webcam (for camera mode) OR ESP32 + Flex Sensors (for glove mode).

### Installation

1.  **Clone the repository**

    ```bash
    git clone https://github.com/JAlavindu/sign-language-to-text-speech.git
    cd sign-language-to-text-speech
    ```

2.  **Set up the environment**

    ```powershell
    .\setup_ml.bat
    ```

3.  **Install additional dependencies** (for Camera & Sensors)
    ```powershell
    .\venv\Scripts\Activate.ps1
    pip install opencv-python mediapipe pyttsx3 bleak
    ```

## ğŸ® Usage Guide

### Phase 1: The "Eyes" (Camera System)

1.  **Train the Model** (If you haven't yet):
    ```powershell
    python ml-model/3_train_model.py
    ```
2.  **Run Real-Time Recognition**:
    ```powershell
    python ml-model/7_realtime_camera.py
    ```
    - **Spacebar**: Speak the current sentence.
    - **Backspace**: Clear the sentence.
    - **Q**: Quit.

### Phase 2: The "Feel" (Glove System)

1.  **Upload Firmware**: Flash `firmware/sensor_streamer/sensor_streamer.ino` to your ESP32.
2.  **Collect Training Data**:
    ```powershell
    python ml-model/8_collect_sensor_data.py
    ```
    - Follow the prompts to record sensor data for each sign.

## ğŸ—ºï¸ Roadmap

- [x] **Hardware Design**: Parts list and wiring diagrams complete.
- [x] **ML Pipeline**: Data exploration, processing, and training scripts ready.
- [x] **Camera System**: Real-time detection, temporal smoothing, and TTS integration.
- [x] **Glove Firmware**: BLE streaming implemented.
- [x] **Sensor Collection**: Python script to record glove data.
- [ ] **Sensor Model**: Train LSTM/CNN model on sensor data.
- [ ] **Fusion**: Combine Camera + Glove predictions for maximum accuracy.
- [ ] **Mobile App**: Port inference to a mobile application.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is open-source and available under the MIT License.
