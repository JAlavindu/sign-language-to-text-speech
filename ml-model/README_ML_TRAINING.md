# Complete ML Model Training Guide

## ASL Sign Language Recognition from Images

This guide will walk you through building a high-accuracy CNN model for ASL alphabet recognition (A-Z + 0-9 = 36 classes).

---

## üìä Dataset Analysis

### Your Datasets:

1. **SignAlphaSet**:

   - 26 folders (A-Z uppercase)
   - ~1000 images per letter
   - Total: ~26,000 images
   - Format: `{Letter}_{number}.jpg`

2. **asl_dataset**:
   - 36 folders (a-z lowercase + 0-9 digits)
   - Multiple angles (top, bottom, left, right, different hands)
   - ~70 images per class
   - Total: ~2,500 images
   - Format: `hand{N}_{letter}_{angle}_seg_{N}_cropped.jpeg`

### Strategy:

- **Primary dataset**: SignAlphaSet (larger, more diverse)
- **Augmentation dataset**: asl_dataset (adds viewpoint variety)
- **Combined total**: ~28,500 images for robust training

---

## üéØ Model Architecture Strategy

### Approach: Transfer Learning + Custom Layers

We'll use **MobileNetV2** as the base (lightweight, perfect for mobile/ESP32 deployment):

```
Input Image (224x224x3)
        ‚Üì
MobileNetV2 (pretrained on ImageNet, frozen)
        ‚Üì
Global Average Pooling
        ‚Üì
Dense Layer (256 units) + Dropout(0.5)
        ‚Üì
Dense Layer (128 units) + Dropout(0.3)
        ‚Üì
Output Layer (36 units, softmax)
```

**Why MobileNetV2?**

- Small size (~14MB) - deployable on mobile/embedded
- Fast inference (~20-50ms on mobile)
- Excellent accuracy for hand gesture recognition
- Can be quantized to 4MB for ESP32

**Alternative architectures** (if you want to experiment):

- **EfficientNetB0**: Better accuracy, slightly larger
- **Custom CNN**: Lighter but needs more training data
- **ResNet50**: Best accuracy but too large for embedded

---

## üìã Step-by-Step Training Process

### **STEP 1: Environment Setup**

#### Create Python Virtual Environment

```powershell
cd "e:\UNI sub\ICT\3rd yr\HCI\sign-language-glove"
python -m venv venv
.\venv\Scripts\Activate.ps1
```

#### Install Required Libraries

```powershell
pip install --upgrade pip
pip install tensorflow==2.15.0
pip install numpy==1.24.3
pip install pandas==2.1.0
pip install matplotlib==3.8.0
pip install seaborn==0.13.0
pip install scikit-learn==1.3.0
pip install pillow==10.0.0
pip install opencv-python==4.8.0
pip install tqdm
pip install jupyter
```

#### Verify Installation

```powershell
python -c "import tensorflow as tf; print('TensorFlow version:', tf.__version__); print('GPU available:', tf.config.list_physical_devices('GPU'))"
```

---

### **STEP 2: Data Preparation & Exploration**

#### Run `1_data_exploration.py`

This script will:

- Count images in each dataset
- Check class distribution
- Visualize sample images
- Identify corrupted/missing images
- Generate dataset statistics

**What to look for:**

- ‚úÖ All 36 classes present (A-Z, 0-9)
- ‚úÖ Balanced distribution (~800-1000 images per class)
- ‚úÖ Image quality and consistency
- ‚ùå Corrupted images (will be skipped)
- ‚ùå Extremely imbalanced classes (will need augmentation)

**Action**: Run the script and review the output report

---

### **STEP 3: Data Preprocessing Pipeline**

#### Run `2_prepare_dataset.py`

This script will:

1. **Merge both datasets** into unified structure
2. **Split data**: 70% train, 15% validation, 15% test
3. **Standardize image names** and folder structure
4. **Create metadata CSV** with file paths and labels
5. **Compute dataset statistics** (mean, std for normalization)

**Output structure:**

```
ml-model/datasets/processed/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ A/
‚îÇ   ‚îú‚îÄ‚îÄ B/
‚îÇ   ‚îî‚îÄ‚îÄ ... (36 folders)
‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îî‚îÄ‚îÄ ... (36 folders)
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îî‚îÄ‚îÄ ... (36 folders)
‚îî‚îÄ‚îÄ metadata.csv
```

**Important**: Keep test set completely separate - never use for training!

---

### **STEP 4: Data Augmentation Strategy**

#### Configure augmentation in `3_train_model.py`

We'll apply **realistic augmentations** that match real-world conditions:

```python
ImageDataGenerator(
    rotation_range=20,           # ¬±20¬∞ rotation (hand orientation varies)
    width_shift_range=0.2,       # 20% horizontal shift
    height_shift_range=0.2,      # 20% vertical shift
    shear_range=0.15,           # Slight perspective distortion
    zoom_range=0.2,             # Zoom in/out (distance variation)
    horizontal_flip=False,       # DON'T flip (signs are orientation-specific!)
    brightness_range=[0.8, 1.2], # Lighting variation
    fill_mode='nearest'
)
```

**Why no horizontal flip?** Many ASL signs are directionally important (e.g., 'P' vs 'Q')

**Augmentation increases dataset by ~5-10x** during training

---

### **STEP 5: Model Training (Main Process)**

#### Run `3_train_model.py`

**Training Configuration:**

```python
EPOCHS = 50              # More epochs = better learning (diminishing returns after 50)
BATCH_SIZE = 32          # Adjust based on GPU memory (16/32/64)
LEARNING_RATE = 0.001    # Start with 0.001, will reduce automatically
IMG_SIZE = 224           # MobileNetV2 standard input size
```

**Training Process:**

**Phase 1: Transfer Learning (Epochs 1-30)**

- MobileNetV2 backbone FROZEN (weights locked)
- Only train custom top layers (Dense + Dropout)
- Fast training, prevents overfitting
- Should reach ~85-90% validation accuracy

**Phase 2: Fine-tuning (Epochs 31-50)**

- UNFREEZE last 30 layers of MobileNetV2
- Lower learning rate (0.0001)
- Fine-tune feature extraction for ASL-specific patterns
- Should reach ~95-98% validation accuracy

**Callbacks Used:**

1. **ModelCheckpoint**: Saves best model based on validation accuracy
2. **EarlyStopping**: Stops if no improvement for 10 epochs (prevents overfitting)
3. **ReduceLROnPlateau**: Reduces learning rate when validation plateaus
4. **TensorBoard**: Logs training metrics for visualization
5. **CSV Logger**: Saves epoch-by-epoch metrics

**Expected Timeline:**

- Epoch 1-10: Rapid improvement (70% ‚Üí 85%)
- Epoch 11-30: Steady improvement (85% ‚Üí 92%)
- Epoch 31-40: Fine-tuning (92% ‚Üí 96%)
- Epoch 41-50: Refinement (96% ‚Üí 97-98%)

**Hardware Requirements:**

- **With GPU**: ~2-4 hours total
- **CPU only**: ~8-16 hours total
- **RAM**: Minimum 8GB, recommended 16GB

---

### **STEP 6: Model Evaluation**

#### Run `4_evaluate_model.py`

This will generate comprehensive evaluation:

**1. Test Set Accuracy**

- Overall accuracy on unseen data
- Target: >95% for deployment

**2. Confusion Matrix**

- Shows which signs are confused
- Example: 'M' vs 'N' (similar hand shapes)
- Identifies weak spots for improvement

**3. Per-Class Metrics**

```
Class | Precision | Recall | F1-Score | Support
------|-----------|--------|----------|--------
  A   |   0.98    |  0.97  |   0.98   |   150
  B   |   0.96    |  0.95  |   0.96   |   145
  ...
```

- **Precision**: Of predicted 'A', how many are actually 'A'?
- **Recall**: Of all actual 'A', how many did we find?
- **F1-Score**: Balanced metric (harmonic mean)

**4. Misclassification Analysis**

- Visualize incorrectly classified images
- Understand failure patterns
- Guide improvements

**5. Learning Curves**

- Plot training vs validation accuracy/loss
- Check for overfitting (train >> val accuracy)
- Check for underfitting (both accuracies low)

**Good signs:**

- ‚úÖ Test accuracy close to validation accuracy (within 2%)
- ‚úÖ No single class below 90% accuracy
- ‚úÖ Confusion matrix mostly diagonal

**Warning signs:**

- ‚ùå Large gap between train/val accuracy (overfitting)
- ‚ùå Some classes significantly worse (need more data/augmentation)
- ‚ùå Consistent confusion between specific pairs (needs better features)

---

### **STEP 7: Model Optimization for Deployment**

#### Run `5_optimize_model.py`

**Optimization Techniques:**

**1. TensorFlow Lite Conversion**
Converts full TensorFlow model to lightweight mobile format:

- **Original size**: ~14 MB
- **TFLite size**: ~14 MB (same size, faster inference)
- **Quantized TFLite**: ~4 MB (8-bit quantization)

**2. Quantization (8-bit)**
Converts 32-bit floats to 8-bit integers:

- 4x size reduction
- 2-4x faster inference
- Minimal accuracy loss (<1%)
- Perfect for mobile/embedded

**3. Pruning (Optional)**
Removes unnecessary neural connections:

- Additional 30-50% size reduction
- Slightly more accuracy loss
- Good for extremely constrained devices

**4. Knowledge Distillation (Advanced)**
Train smaller "student" model to mimic larger "teacher":

- Can achieve 80% of accuracy with 10% of size
- Requires separate training process

**Outputs:**

```
ml-model/models/
‚îú‚îÄ‚îÄ asl_model.h5                    # Original Keras model (14MB)
‚îú‚îÄ‚îÄ asl_model.tflite               # TFLite model (14MB)
‚îú‚îÄ‚îÄ asl_model_quantized.tflite     # Quantized (4MB) ‚Üê Use this for mobile
‚îî‚îÄ‚îÄ asl_model_esp32.tflite         # Further optimized for ESP32 (3MB)
```

**Which model to use?**

- **Smartphone app**: `asl_model_quantized.tflite` (best balance)
- **ESP32**: `asl_model_esp32.tflite` (smallest, may need external camera)
- **High accuracy needs**: `asl_model.h5` (original)

---

### **STEP 8: Real-Time Testing & Inference**

#### Run `6_test_realtime.py`

Test your model with:

1. **Webcam real-time recognition**
2. **Single image prediction**
3. **Batch prediction on test images**

**Real-time performance targets:**

- **Latency**: <100ms per frame (10 FPS minimum)
- **Accuracy**: Match test set accuracy
- **Robustness**: Works with different lighting, backgrounds, hands

**Test scenarios:**

- ‚úÖ Different lighting conditions
- ‚úÖ Different backgrounds (plain, cluttered)
- ‚úÖ Different hand sizes/skin tones
- ‚úÖ Slight occlusions
- ‚úÖ Different distances from camera

---

## üéì Understanding Training Metrics

### Key Metrics to Monitor:

**1. Training Loss vs Validation Loss**

```
Good Pattern (Converging):
Loss ‚îÇ  ‚ï≤ Train
     ‚îÇ   ‚ï≤___________
     ‚îÇ    ‚ï≤ Val
     ‚îÇ     ‚ï≤________
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs

Bad Pattern (Overfitting):
Loss ‚îÇ  ‚ï≤ Train
     ‚îÇ   ‚ï≤____________
     ‚îÇ
     ‚îÇ       ‚ï± Val (increasing!)
     ‚îÇ      ‚ï±
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs
```

**2. Accuracy Progression**

- **Epoch 1-5**: 40-70% (learning basics)
- **Epoch 10-20**: 80-90% (refining features)
- **Epoch 30-50**: 95-98% (fine details)

**3. Learning Rate Schedule**

- Start: 0.001 (fast learning)
- Epoch 20: 0.0005 (slower, more precise)
- Epoch 35: 0.0001 (fine-tuning)

---

## üîß Troubleshooting Common Issues

### Issue 1: Low Training Accuracy (<70%)

**Causes:**

- Learning rate too high/low
- Model too simple
- Data quality issues

**Solutions:**

1. Adjust learning rate (try 0.0001 or 0.01)
2. Check data preprocessing
3. Visualize training samples (are labels correct?)
4. Try different model architecture

---

### Issue 2: Overfitting (Train 95%, Val 75%)

**Causes:**

- Not enough data augmentation
- Model too complex
- Not enough regularization

**Solutions:**

1. Increase dropout rates (0.5 ‚Üí 0.6)
2. Add more augmentation
3. Use L2 regularization
4. Reduce model complexity
5. Collect more diverse data

---

### Issue 3: Specific Classes Performing Poorly

**Causes:**

- Class imbalance
- Visually similar to other classes
- Poor quality images

**Solutions:**

1. Collect more samples for weak classes
2. Apply class-specific augmentation
3. Use class weights in loss function
4. Review and clean mislabeled images

---

### Issue 4: Slow Training

**Causes:**

- No GPU acceleration
- Batch size too small
- Image size too large

**Solutions:**

1. Reduce image size (224 ‚Üí 128)
2. Increase batch size (32 ‚Üí 64)
3. Use mixed precision training
4. Enable GPU if available

---

### Issue 5: Model Too Large for Deployment

**Causes:**

- Using full-resolution model
- No compression applied

**Solutions:**

1. Apply quantization (32-bit ‚Üí 8-bit)
2. Use pruning
3. Try knowledge distillation
4. Use smaller base model (MobileNetV2 ‚Üí V3-Small)

---

## üìà Expected Results & Benchmarks

### Target Accuracies:

- **Training accuracy**: 97-99%
- **Validation accuracy**: 95-97%
- **Test accuracy**: 95-97%
- **Real-world accuracy**: 90-95% (with good lighting/angle)

### Model Sizes:

- **Original**: 14 MB
- **Quantized**: 4 MB
- **Pruned + Quantized**: 2-3 MB

### Inference Speed:

- **Desktop CPU**: ~20-30ms
- **Smartphone**: ~50-100ms
- **ESP32-S3**: ~200-500ms (with external camera)

---

## üöÄ Next Steps After Training

1. **Integration with glove hardware**

   - Mount camera on glove or nearby
   - Stream images to phone via BLE
   - Run inference on phone
   - Convert prediction to speech

2. **Continuous improvement**

   - Collect user data (with permission)
   - Retrain periodically
   - A/B test model versions
   - Monitor real-world accuracy

3. **Feature additions**
   - Word prediction (add language model)
   - Gesture sequences (A-P-P-L-E ‚Üí "apple")
   - Custom sign learning
   - Multi-hand support

---

## üìö Additional Resources

### Papers to Read:

1. "MobileNets: Efficient CNNs for Mobile Vision Applications"
2. "Rethinking the Inception Architecture for Computer Vision"
3. "Deep Residual Learning for Image Recognition"

### Datasets:

- ASL Alphabet Dataset (Kaggle)
- ASL-LEX Dataset (linguistic features)
- MS-ASL Dataset (video sequences)

### Tools:

- **TensorBoard**: Visualize training (`tensorboard --logdir logs/`)
- **Netron**: Visualize model architecture
- **TensorFlow Lite Converter**: Model optimization

---

## ‚úÖ Quick Start Checklist

Before you begin:

- [ ] Python 3.10+ installed
- [ ] Virtual environment created and activated
- [ ] All dependencies installed (`pip install -r requirements.txt`)
- [ ] Datasets located in correct folders
- [ ] GPU drivers installed (if using GPU)
- [ ] At least 20GB free disk space
- [ ] 8GB+ RAM available

**Estimated total time**: 6-8 hours (including training)

---

## üéØ Success Criteria

Your model is ready for deployment when:

1. ‚úÖ Test accuracy > 95%
2. ‚úÖ No class below 90% F1-score
3. ‚úÖ Model size < 5MB (quantized)
4. ‚úÖ Inference time < 100ms on target device
5. ‚úÖ Real-world testing shows consistent results
6. ‚úÖ Confusion matrix shows clear diagonal pattern
7. ‚úÖ Learning curves show convergence without overfitting

**You're now ready to build a production-quality ASL recognition system!** üöÄ
